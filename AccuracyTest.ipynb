{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AccuracyTest.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOX6pb5j5WNvrUQOsR5MFu/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"a-VMUO2F9y90"},"source":["from torch.autograd import Variable\n","import numpy as np\n","import tensorflow as tf\n","import time, math, glob\n","import scipy.io as sio\n","import torch\n","import torch.nn as nn\n","from math import sqrt\n","import argparse, os\n","import torch\n","import random\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.utils.data as data\n","import h5py\n","\n","from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import os\n","import copy\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0WiFSPu90Fv"},"source":["import numpy as np\n","import torch\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","from PIL.ImageOps import colorize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bo2x_Ije92DZ"},"source":["import h5py as h5py\n","import numpy as np\n","import tensorflow as tf\n","from sklearn import metrics\n","import torchvision.models as models\n","from PIL import Image\n","import imageio\n","import matplotlib.pyplot as plt\n","import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o3jU8u3w95I2","executionInfo":{"status":"ok","timestamp":1630287842177,"user_tz":-480,"elapsed":21918,"user":{"displayName":"胡应东","photoUrl":"","userId":"16123890978426666287"}},"outputId":"1384609d-35de-4c31-cff1-3fa4d9a0ecf6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = \"/content/drive/MyDrive\"\n","os.chdir(path)\n","os.listdir(path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Colab Notebooks',\n"," 'content',\n"," 'drive',\n"," 'train',\n"," 'validation',\n"," 'Cifar10_Aug',\n"," '0',\n"," '5',\n"," 'tensorboard',\n"," 'Cifar10_split',\n"," 'TIP Moire Pattern Removal',\n"," 'data',\n"," 'checkpoint_old',\n"," 'checkpoint']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLZJ1-9T9_aK","executionInfo":{"status":"ok","timestamp":1630287938664,"user_tz":-480,"elapsed":96496,"user":{"displayName":"胡应东","photoUrl":"","userId":"16123890978426666287"}},"outputId":"a1101340-a694-4563-fd86-526af4ac6a41"},"source":["data_transforms = {\n","    'train': transforms.Compose([\n","        #ransforms.Resize(128),\n","        transforms.RandomRotation(20),\n","        transforms.ColorJitter(),\n","        #transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        #transforms.Resize(128),\n","        #transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        #transforms.Resize(128),\n","        #transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","# data_dir = '/content/drive/MyDrive/Cifar10-VDSR(256)_split'\n","data_dir = '/content/drive/MyDrive/Cifar10_split'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val', 'test']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n","                                             shuffle=True, num_workers=8, drop_last=True)\n","              for x in ['train', 'val', 'test']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eVofoBUR-ASK","executionInfo":{"status":"ok","timestamp":1630295149509,"user_tz":-480,"elapsed":604,"user":{"displayName":"胡应东","photoUrl":"","userId":"16123890978426666287"}}},"source":["class Net2(torch.nn.Module):\n","    def __init__(self, num_channels=3, upscale_factor=2, d=64, s=12, m=4):\n","        super(Net2, self).__init__()\n","\n","        self.first_part = nn.Sequential(nn.Conv2d(in_channels=num_channels, out_channels=d, kernel_size=5, stride=1, padding=2),\n","                                        nn.PReLU())\n","\n","        self.layers = []\n","        self.layers.append(nn.Sequential(nn.Conv2d(in_channels=d, out_channels=s, kernel_size=1, stride=1, padding=0),\n","                                         nn.PReLU()))\n","        for _ in range(m):\n","            self.layers.append(nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, stride=1, padding=1))\n","        self.layers.append(nn.PReLU())\n","        self.layers.append(nn.Sequential(nn.Conv2d(in_channels=s, out_channels=d, kernel_size=1, stride=1, padding=0),\n","                                         nn.PReLU()))\n","\n","        self.mid_part = torch.nn.Sequential(*self.layers)\n","\n","        # Deconvolution\n","        self.last_part = nn.ConvTranspose2d(in_channels=d, out_channels=num_channels, kernel_size=9, stride=upscale_factor, padding=3, output_padding=1)\n","\n","    def forward(self, x):\n","        out = self.first_part(x)\n","        out = self.mid_part(out)\n","        out = self.last_part(out)\n","        return out\n","\n","    def weight_init(self, mean=0.0, std=0.02):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                m.weight.data.normal_(mean, std)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            if isinstance(m, nn.ConvTranspose2d):\n","                m.weight.data.normal_(0.0, 0.0001)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","\n","class Conv_ReLU_Block(nn.Module):\n","    def __init__(self):\n","        super(Conv_ReLU_Block, self).__init__()\n","        self.conv = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","        \n","    def forward(self, x):\n","        return self.relu(self.conv(x))\n","        \n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.residual_layer = self.make_layer(Conv_ReLU_Block, 18)\n","        self.input = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.output = nn.Conv2d(in_channels=128, out_channels=3, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","    \n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, sqrt(2. / n))\n","                \n","    def make_layer(self, block, num_of_layer):\n","        layers = []\n","        for _ in range(num_of_layer):\n","            layers.append(block())\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.relu(self.input(x))\n","        out = self.residual_layer(out)\n","        out = self.output(out)\n","        out = torch.add(out,residual)\n","        return residual,out\n","\n","def conv_layer(chann_in, chann_out, k_size, p_size):\n","    layer = nn.Sequential(\n","        nn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),\n","        nn.BatchNorm2d(chann_out),\n","        nn.ReLU()\n","    )\n","    return layer\n","\n","def vgg_fc_layer(size_in, size_out):\n","    layer = nn.Sequential(\n","        nn.Linear(size_in, size_out),\n","        nn.BatchNorm1d(size_out),\n","        nn.ReLU()\n","    )\n","    return layer\n","\n","def vgg_conv_block(in_list, out_list, k_list, p_list, pooling_k, pooling_s):\n","\n","    layers = [conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) for i in range(len(in_list))]\n","    layers += [nn.MaxPool2d(kernel_size=pooling_k, stride=pooling_s)]\n","    return nn.Sequential(*layers)\n","\n","class SDCNN(nn.Module):\n","    def __init__(self, n_classes=10):\n","        super(SDCNN, self).__init__()\n","\n","        # Conv blocks (BatchNorm + ReLU activation added in each block)\n","        self.layer1 = vgg_conv_block([3,32], [32,32], [3,3], [1,1], 2, 2)\n","        self.layer2 = vgg_conv_block([32,64], [64,64], [3,3], [1,1], 2, 2)\n","\n","        # FC layers\n","        self.layer3 = vgg_fc_layer(16*16*64, 512)  # 4096->smaller\n","\n","        # Final layer\n","        self.layer4 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        # 从这里开始\n","        out = self.layer1(x)\n","        features = self.layer2(out)\n","        out = features.view(out.size(0), -1)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","\n","        return out\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSuA-mOa-DEO","executionInfo":{"status":"ok","timestamp":1630295162298,"user_tz":-480,"elapsed":10426,"user":{"displayName":"胡应东","photoUrl":"","userId":"16123890978426666287"}},"outputId":"97686a69-7b2d-4453-f91b-ea594a29d61a"},"source":["parser = argparse.ArgumentParser(description=\"PyTorch VDSR\")\n","parser.add_argument(\"--nEpochs\", type=int, default=50, help=\"Number of epochs to train for\")\n","parser.add_argument(\"--lr\", type=float, default=0.00001, help=\"Learning Rate. Default=0.1\")\n","parser.add_argument(\"--step\", type=int, default=10, help=\"Sets the learning rate to the initial LR decayed by momentum every n epochs, Default: n=10\")\n","parser.add_argument(\"--cuda\", action=\"store_true\", help=\"Use cuda?\")\n","parser.add_argument(\"--resume\", default=\"\",type=str, help=\"Path to checkpoint (default: none)\")\n","parser.add_argument(\"--start-epoch\", default=1, type=int, help=\"Manual epoch number (useful on restarts)\")\n","parser.add_argument(\"--clip\", type=float, default=0.4, help=\"Clipping Gradients. Default=0.4\")\n","parser.add_argument(\"--threads\", type=int, default=1, help=\"Number of threads for data loader to use, Default: 1\")\n","parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"Momentum, Default: 0.9\")\n","parser.add_argument(\"--weight-decay\", \"--wd\", default=1e-4, type=float, help=\"Weight decay, Default: 1e-4\")\n","parser.add_argument('--pretrained_SR', default='/content/drive/MyDrive/checkpoint/model_FSRCNN_CNET_*2_epoch_30.pth', type=str, help='path to pretrained model (default: none)')\n","parser.add_argument('--pretrained_TL', default='/content/drive/MyDrive/checkpoint/TransferLearning/model_FSRCNN_CNET_*2_epoch_30.pth', type=str, help='path to pretrained model (default: none)')\n","parser.add_argument(\"--gpus\", default=\"0\", type=str, help=\"gpu ids (default: 0)\")\n","parser.add_argument(\"--pretrained_SR_num\", default=1, type=int, help=\"numbers of epochs that have been trained\")\n","parser.add_argument(\"--pretrained_TL_num\", default=1, type=int, help=\"numbers of epochs that have been trained\")\n","parser.add_argument(\"--SRtrain\", default=True, help=\"if train the super resolution network\")\n","parser.add_argument(\"--TLtrain\", default=True, help=\"if train the trainsfer learning network\")\n","parser.add_argument(\"--SR_used\", default=True, help=\"if use the SR method thought the pipeline\")\n","\n","opt = parser.parse_args(args=[])\n","\n","SRmodel = Net2()\n","CNET = SDCNN()\n","SRmodel = SRmodel.cuda()\n","CNET = CNET.cuda()\n","\n","if opt.pretrained_SR:\n","    if os.path.isfile(opt.pretrained_SR):\n","        print(\"=> loading SR model '{}'\".format(opt.pretrained_SR))\n","        weights = torch.load(opt.pretrained_SR)\n","        SRmodel.load_state_dict(weights['model'].state_dict())\n","    else:\n","        print(\"=> no model found at '{}'\".format(opt.pretrained_SR))  \n","    \n","if opt.pretrained_TL:\n","    if os.path.isfile(opt.pretrained_TL):\n","        print(\"=> loading TL model '{}'\".format(opt.pretrained_TL))\n","        weights = torch.load(opt.pretrained_TL)\n","        CNET.load_state_dict(weights['model'].state_dict())\n","    else:\n","        print(\"=> no model found at '{}'\".format(opt.pretrained_TL))\n","\n","correct = 0\n","total = 0\n","for images, labels in dataloaders['test']:\n","    images = images.cuda()\n","    if opt.SR_used:\n","      #_,images = SRmodel(images)\n","      images = SRmodel(images)\n","    outputs = CNET(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted.cpu() == labels).sum()\n","    print(\"avg acc: %f\" % (100* correct/total))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["=> loading SR model '/content/drive/MyDrive/checkpoint/model_FSRCNN_CNET_*2_epoch_30.pth'\n","=> loading TL model '/content/drive/MyDrive/checkpoint/TransferLearning/model_FSRCNN_CNET_*2_epoch_30.pth'\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["avg acc: 87.500000\n","avg acc: 78.125000\n","avg acc: 77.083336\n","avg acc: 76.562500\n","avg acc: 80.000000\n","avg acc: 82.291664\n","avg acc: 82.142860\n","avg acc: 78.125000\n","avg acc: 79.166664\n","avg acc: 78.750000\n","avg acc: 80.113640\n","avg acc: 80.729164\n","avg acc: 78.846153\n","avg acc: 78.571426\n","avg acc: 78.750000\n","avg acc: 78.515625\n","avg acc: 77.941177\n","avg acc: 77.777779\n","avg acc: 77.631577\n","avg acc: 78.125000\n","avg acc: 77.976189\n","avg acc: 78.125000\n","avg acc: 77.989128\n","avg acc: 78.125000\n","avg acc: 78.000000\n","avg acc: 77.884613\n","avg acc: 77.083336\n","avg acc: 77.678574\n","avg acc: 77.801727\n","avg acc: 77.708336\n","avg acc: 78.024193\n","avg acc: 77.929688\n","avg acc: 77.840912\n","avg acc: 77.757355\n","avg acc: 77.678574\n","avg acc: 78.298615\n","avg acc: 77.871620\n","avg acc: 77.960526\n","avg acc: 78.044868\n","avg acc: 77.812500\n","avg acc: 77.896339\n","avg acc: 77.827377\n","avg acc: 78.197678\n","avg acc: 78.267044\n","avg acc: 78.750000\n","avg acc: 78.804344\n","avg acc: 78.723404\n","avg acc: 78.906250\n","avg acc: 78.954079\n","avg acc: 79.000000\n","avg acc: 79.044121\n","avg acc: 79.206734\n","avg acc: 79.363205\n","avg acc: 79.282410\n","avg acc: 79.090912\n","avg acc: 79.017860\n","avg acc: 79.166664\n","avg acc: 78.879311\n","avg acc: 78.389832\n","avg acc: 78.437500\n","avg acc: 78.278687\n","avg acc: 78.528229\n","avg acc: 78.472221\n","avg acc: 78.222656\n","avg acc: 77.980766\n","avg acc: 78.125000\n","avg acc: 78.078362\n","avg acc: 78.125000\n","avg acc: 78.170288\n","avg acc: 78.214287\n","avg acc: 78.433098\n","avg acc: 78.298615\n","avg acc: 77.996574\n","avg acc: 78.125000\n","avg acc: 78.000000\n","avg acc: 78.125000\n","avg acc: 78.165581\n","avg acc: 77.884613\n","avg acc: 78.006332\n","avg acc: 77.968750\n","avg acc: 77.854935\n","avg acc: 77.667686\n","avg acc: 77.710846\n","avg acc: 77.901787\n","avg acc: 77.941177\n","avg acc: 77.979652\n","avg acc: 77.945404\n","avg acc: 77.840912\n","avg acc: 77.598312\n","avg acc: 77.569443\n","avg acc: 77.609894\n","avg acc: 77.717392\n","avg acc: 77.620964\n","avg acc: 77.659576\n","avg acc: 77.302635\n","avg acc: 77.343750\n","avg acc: 77.512886\n","avg acc: 77.551018\n","avg acc: 77.588387\n","avg acc: 77.562500\n","avg acc: 77.537132\n","avg acc: 77.328430\n","avg acc: 77.305824\n","avg acc: 77.343750\n","avg acc: 77.380951\n","avg acc: 77.299530\n","avg acc: 77.394859\n","avg acc: 77.430557\n","avg acc: 77.522934\n","avg acc: 77.386360\n","avg acc: 77.477478\n","avg acc: 77.455360\n","avg acc: 77.488937\n","avg acc: 77.686401\n","avg acc: 77.771736\n","avg acc: 77.747849\n","avg acc: 77.831200\n","avg acc: 77.701271\n","avg acc: 77.783615\n","avg acc: 77.708336\n","avg acc: 77.789253\n","avg acc: 77.766396\n","avg acc: 77.743904\n","avg acc: 77.721771\n","avg acc: 77.750000\n","avg acc: 77.728172\n","avg acc: 77.805115\n","avg acc: 77.734375\n","avg acc: 77.810074\n","avg acc: 77.836540\n","avg acc: 77.862595\n","avg acc: 77.840912\n","avg acc: 77.819550\n","avg acc: 77.845146\n","avg acc: 77.916664\n","avg acc: 77.895218\n","avg acc: 77.782845\n","avg acc: 77.762680\n","avg acc: 77.787773\n","avg acc: 77.678574\n","avg acc: 77.615250\n","avg acc: 77.552818\n","avg acc: 77.447556\n","avg acc: 77.473961\n","avg acc: 77.500000\n","avg acc: 77.611298\n","avg acc: 77.721092\n","avg acc: 77.787163\n","avg acc: 77.726509\n","avg acc: 77.791664\n","avg acc: 77.814568\n","avg acc: 77.837173\n","avg acc: 77.818626\n","avg acc: 77.922081\n","avg acc: 77.862900\n","avg acc: 77.964745\n","avg acc: 77.906052\n","avg acc: 77.966774\n","avg acc: 77.987419\n","avg acc: 77.968750\n","avg acc: 78.027954\n","avg acc: 78.009262\n","avg acc: 77.799080\n","avg acc: 77.820122\n","avg acc: 77.765152\n","avg acc: 77.823792\n","avg acc: 77.844315\n","avg acc: 77.976189\n","avg acc: 77.884613\n","avg acc: 77.941177\n","avg acc: 77.923973\n","avg acc: 77.906975\n","avg acc: 77.854050\n","avg acc: 77.765808\n","avg acc: 77.785713\n","avg acc: 77.876419\n","avg acc: 77.895477\n","avg acc: 77.844101\n","avg acc: 77.828209\n","avg acc: 77.916664\n","avg acc: 77.866020\n","avg acc: 77.918953\n","avg acc: 77.903008\n","avg acc: 77.717392\n","avg acc: 77.702705\n","avg acc: 77.587364\n","avg acc: 77.540108\n","avg acc: 77.593086\n","avg acc: 77.546295\n","avg acc: 77.532898\n","avg acc: 77.519630\n","avg acc: 77.571617\n","avg acc: 77.493523\n","avg acc: 77.480667\n","avg acc: 77.435898\n","avg acc: 77.359695\n","avg acc: 77.379440\n","avg acc: 77.367424\n","avg acc: 77.418343\n","avg acc: 77.500000\n","avg acc: 77.456467\n","avg acc: 77.413368\n","avg acc: 77.463051\n","avg acc: 77.450981\n","avg acc: 77.469513\n","avg acc: 77.518204\n","avg acc: 77.566422\n","avg acc: 77.644234\n","avg acc: 77.661484\n","avg acc: 77.708336\n","avg acc: 77.725121\n","avg acc: 77.712265\n","avg acc: 77.787560\n","avg acc: 77.832947\n","avg acc: 77.819771\n","avg acc: 77.748840\n","avg acc: 77.851379\n","avg acc: 77.895645\n","avg acc: 77.882423\n","avg acc: 77.812500\n","avg acc: 77.856339\n","avg acc: 77.927925\n","avg acc: 77.914795\n","avg acc: 77.845985\n","avg acc: 77.777779\n","avg acc: 77.848450\n","avg acc: 77.863434\n","avg acc: 77.878288\n","avg acc: 77.865723\n","avg acc: 77.907608\n","avg acc: 77.922081\n","avg acc: 77.909485\n","avg acc: 77.923820\n","avg acc: 77.938034\n","avg acc: 77.978722\n","avg acc: 77.992584\n","avg acc: 77.953583\n","avg acc: 77.967438\n","avg acc: 77.981171\n","avg acc: 77.916664\n","avg acc: 77.878632\n","avg acc: 77.840912\n","avg acc: 77.854935\n","avg acc: 77.843239\n","avg acc: 77.857140\n","avg acc: 77.845528\n","avg acc: 77.859314\n","avg acc: 77.847786\n","avg acc: 77.761047\n","avg acc: 77.750000\n","avg acc: 77.763947\n","avg acc: 77.653770\n","avg acc: 77.593872\n","avg acc: 77.632874\n","avg acc: 77.647057\n","avg acc: 77.661133\n","avg acc: 77.723732\n","avg acc: 77.785851\n","avg acc: 77.775093\n","avg acc: 77.716347\n","avg acc: 77.705940\n","avg acc: 77.743324\n","avg acc: 77.732887\n","avg acc: 77.627838\n","avg acc: 77.641510\n","avg acc: 77.678574\n","avg acc: 77.668541\n","avg acc: 77.588623\n","avg acc: 77.602234\n","avg acc: 77.638885\n","avg acc: 77.629150\n","avg acc: 77.619484\n","avg acc: 77.655678\n","avg acc: 77.645988\n","avg acc: 77.681816\n","avg acc: 77.694748\n","avg acc: 77.639893\n","avg acc: 77.675362\n","avg acc: 77.755379\n","avg acc: 77.700890\n","avg acc: 77.713524\n","avg acc: 77.703903\n","avg acc: 77.650177\n","avg acc: 77.596832\n","avg acc: 77.521927\n","avg acc: 77.534966\n","avg acc: 77.526131\n","avg acc: 77.560760\n","avg acc: 77.573532\n","avg acc: 77.543106\n","avg acc: 77.577316\n","avg acc: 77.589897\n","avg acc: 77.602386\n","avg acc: 77.572281\n","avg acc: 77.605934\n","avg acc: 77.597130\n","avg acc: 77.672562\n","avg acc: 77.642616\n","avg acc: 77.696487\n","avg acc: 77.687500\n","avg acc: 77.720100\n","avg acc: 77.731789\n","avg acc: 77.743401\n","avg acc: 77.734375\n","avg acc: 77.643440\n","avg acc: 77.614380\n","avg acc: 77.626221\n","avg acc: 77.577110\n","avg acc: 77.609222\n","avg acc: 77.580643\n","avg acc: 77.612541\n","avg acc: 77.624199\n","avg acc: 77.595848\n","avg acc: 77.587578\n","avg acc: 77.559525\n","avg acc: 77.551422\n","avg acc: 77.563095\n","avg acc: 77.535378\n","avg acc: 77.547020\n","avg acc: 77.500000\n","avg acc: 77.531151\n","avg acc: 77.562111\n","avg acc: 77.554176\n","avg acc: 77.584877\n","avg acc: 77.538460\n","avg acc: 77.549843\n","avg acc: 77.561165\n","avg acc: 77.534302\n","avg acc: 77.526596\n","avg acc: 77.537880\n","avg acc: 77.549095\n","avg acc: 77.484940\n","avg acc: 77.477478\n","avg acc: 77.488770\n","avg acc: 77.500000\n","avg acc: 77.529762\n","avg acc: 77.522255\n","avg acc: 77.533287\n","avg acc: 77.507378\n","avg acc: 77.536766\n","avg acc: 77.547653\n","avg acc: 77.595032\n","avg acc: 77.605682\n","avg acc: 77.634445\n","avg acc: 77.644928\n","avg acc: 77.637283\n","avg acc: 77.647697\n","avg acc: 77.640083\n","avg acc: 77.632523\n","avg acc: 77.607140\n","avg acc: 77.564102\n","avg acc: 77.556816\n","avg acc: 77.620399\n","avg acc: 77.630653\n","avg acc: 77.658447\n","avg acc: 77.686096\n","avg acc: 77.731094\n","avg acc: 77.740921\n","avg acc: 77.733284\n","avg acc: 77.725693\n","avg acc: 77.700829\n","avg acc: 77.641571\n","avg acc: 77.599861\n","avg acc: 77.609894\n","avg acc: 77.602737\n","avg acc: 77.527321\n","avg acc: 77.571526\n","avg acc: 77.598503\n","avg acc: 77.642273\n","avg acc: 77.668922\n","avg acc: 77.644882\n","avg acc: 77.620964\n","avg acc: 77.647453\n","avg acc: 77.640373\n","avg acc: 77.599998\n","avg acc: 77.609711\n","avg acc: 77.602783\n","avg acc: 77.645500\n","avg acc: 77.556068\n","avg acc: 77.582237\n","avg acc: 77.591866\n","avg acc: 77.585075\n","avg acc: 77.594650\n","avg acc: 77.636719\n","avg acc: 77.532471\n","avg acc: 77.558289\n","avg acc: 77.567833\n","avg acc: 77.609535\n","avg acc: 77.618896\n","avg acc: 77.564102\n","avg acc: 77.573532\n","avg acc: 77.614799\n","avg acc: 77.624046\n","avg acc: 77.617386\n","avg acc: 77.563293\n","avg acc: 77.525253\n","avg acc: 77.550377\n","avg acc: 77.575378\n","avg acc: 77.615913\n","avg acc: 77.609375\n","avg acc: 77.556107\n","avg acc: 77.565300\n","avg acc: 77.558929\n","avg acc: 77.552597\n","avg acc: 77.546295\n","avg acc: 77.555420\n","avg acc: 77.549141\n","avg acc: 77.542892\n","avg acc: 77.536674\n","avg acc: 77.530487\n","avg acc: 77.554741\n","avg acc: 77.548546\n","avg acc: 77.587776\n","avg acc: 77.611717\n","avg acc: 77.620483\n","avg acc: 77.644234\n","avg acc: 77.607910\n","avg acc: 77.616631\n","avg acc: 77.625298\n","avg acc: 77.633926\n","avg acc: 77.672211\n","avg acc: 77.695496\n","avg acc: 77.703903\n","avg acc: 77.668045\n","avg acc: 77.617645\n","avg acc: 77.655518\n","avg acc: 77.620026\n","avg acc: 77.643105\n","avg acc: 77.636948\n","avg acc: 77.659882\n","avg acc: 77.595711\n","avg acc: 77.604164\n","avg acc: 77.583717\n","avg acc: 77.592163\n","avg acc: 77.600578\n","avg acc: 77.565941\n","avg acc: 77.588676\n","avg acc: 77.568497\n","avg acc: 77.562645\n","avg acc: 77.585228\n","avg acc: 77.607712\n","avg acc: 77.615952\n","avg acc: 77.567719\n","avg acc: 77.576012\n","avg acc: 77.570221\n","avg acc: 77.536438\n","avg acc: 77.502800\n","avg acc: 77.497208\n","avg acc: 77.463806\n","avg acc: 77.458336\n","avg acc: 77.494453\n","avg acc: 77.488937\n","avg acc: 77.511040\n","avg acc: 77.533043\n","avg acc: 77.527473\n","avg acc: 77.521927\n","avg acc: 77.516411\n","avg acc: 77.483627\n","avg acc: 77.464600\n","avg acc: 77.445656\n","avg acc: 77.453903\n","avg acc: 77.475647\n","avg acc: 77.429802\n","avg acc: 77.438042\n","avg acc: 77.392471\n","avg acc: 77.400749\n","avg acc: 77.382225\n","avg acc: 77.390488\n","avg acc: 77.372070\n","avg acc: 77.367020\n","avg acc: 77.362000\n","avg acc: 77.370232\n","avg acc: 77.365219\n","avg acc: 77.373421\n","avg acc: 77.342102\n","avg acc: 77.337181\n","avg acc: 77.332283\n","avg acc: 77.288177\n","avg acc: 77.322548\n","avg acc: 77.317711\n","avg acc: 77.312889\n","avg acc: 77.308090\n","avg acc: 77.303314\n","avg acc: 77.298553\n","avg acc: 77.306702\n","avg acc: 77.289093\n","avg acc: 77.310059\n","avg acc: 77.305328\n","avg acc: 77.287834\n","avg acc: 77.295921\n","avg acc: 77.303970\n","avg acc: 77.311989\n","avg acc: 77.319977\n","avg acc: 77.327934\n","avg acc: 77.310608\n","avg acc: 77.255547\n","avg acc: 77.225853\n","avg acc: 77.221382\n","avg acc: 77.216934\n","avg acc: 77.187500\n","avg acc: 77.208084\n","avg acc: 77.228584\n","avg acc: 77.236580\n","avg acc: 77.269348\n","avg acc: 77.240097\n","avg acc: 77.272728\n","avg acc: 77.280571\n","avg acc: 77.251480\n","avg acc: 77.222496\n","avg acc: 77.218140\n","avg acc: 77.250488\n","avg acc: 77.258301\n","avg acc: 77.266083\n","avg acc: 77.261673\n","avg acc: 77.281555\n","avg acc: 77.313469\n","avg acc: 77.321083\n","avg acc: 77.352798\n","avg acc: 77.372353\n","avg acc: 77.331734\n","avg acc: 77.363243\n","avg acc: 77.310822\n","avg acc: 77.318359\n","avg acc: 77.313934\n","avg acc: 77.345238\n","avg acc: 77.317017\n","avg acc: 77.312622\n","avg acc: 77.343750\n","avg acc: 77.351135\n","avg acc: 77.358490\n","avg acc: 77.365822\n","avg acc: 77.361374\n","avg acc: 77.345215\n","avg acc: 77.364235\n","avg acc: 77.359810\n","avg acc: 77.355408\n","avg acc: 77.362663\n","avg acc: 77.381508\n","avg acc: 77.353897\n","avg acc: 77.349541\n","avg acc: 77.345192\n","avg acc: 77.352402\n","avg acc: 77.382599\n","avg acc: 77.401192\n","avg acc: 77.385323\n","avg acc: 77.380951\n","avg acc: 77.399452\n","avg acc: 77.372261\n","avg acc: 77.402092\n","avg acc: 77.363640\n","avg acc: 77.382034\n","avg acc: 77.389038\n","avg acc: 77.407326\n","avg acc: 77.402977\n","avg acc: 77.398651\n","avg acc: 77.405579\n","avg acc: 77.434921\n","avg acc: 77.452957\n","avg acc: 77.437386\n","avg acc: 77.399551\n","avg acc: 77.439842\n","avg acc: 77.424377\n","avg acc: 77.453377\n","avg acc: 77.471191\n","avg acc: 77.444687\n","avg acc: 77.429329\n","avg acc: 77.414024\n","avg acc: 77.398766\n","avg acc: 77.416519\n","avg acc: 77.401314\n","avg acc: 77.408058\n","avg acc: 77.414772\n","avg acc: 77.410561\n","avg acc: 77.428139\n","avg acc: 77.456520\n","avg acc: 77.473961\n","avg acc: 77.491333\n","avg acc: 77.476212\n","avg acc: 77.493523\n","avg acc: 77.521553\n","avg acc: 77.527969\n","avg acc: 77.545105\n","avg acc: 77.562180\n","avg acc: 77.568497\n","avg acc: 77.564102\n","avg acc: 77.570389\n","avg acc: 77.555367\n","avg acc: 77.551018\n","avg acc: 77.578522\n","avg acc: 77.584747\n","avg acc: 77.569794\n","avg acc: 77.554901\n","avg acc: 77.571671\n","avg acc: 77.546295\n","avg acc: 77.531509\n","avg acc: 77.537750\n","avg acc: 77.564911\n","avg acc: 77.550171\n","avg acc: 77.545906\n","avg acc: 77.541664\n","avg acc: 77.558235\n","avg acc: 77.564369\n","avg acc: 77.580849\n","avg acc: 77.617966\n","avg acc: 77.634300\n","avg acc: 77.660889\n","avg acc: 77.625618\n","avg acc: 77.652138\n","avg acc: 77.647781\n","avg acc: 77.653687\n","avg acc: 77.659576\n","avg acc: 77.645020\n","avg acc: 77.650894\n","avg acc: 77.646584\n","avg acc: 77.662605\n","avg acc: 77.668427\n","avg acc: 77.684357\n","avg acc: 77.680016\n","avg acc: 77.675690\n","avg acc: 77.671371\n","avg acc: 77.677132\n","avg acc: 77.692924\n","avg acc: 77.698639\n","avg acc: 77.694313\n","avg acc: 77.709999\n"],"name":"stdout"}]}]}