{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "0820_lighter_bicubic.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RuihangZhao/A-SR-preprocessing-module/blob/main/0820_lighter_network(control%20group%2C%20using%20bicubic).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Control group: Our custom classification network with bicubic interpolation as preprocessing SR module\n",
    "==============================================\n",
    "\n",
    "This Jupyter code trains our smaller classification network on Cifar-10 with bicubic interpolation (it's default method as used in image resizing, so we didn't write specific codes to achieve this function) as the control group of image super resolution.\n",
    "\n",
    "We have split the original Cifar-10 dataset into train/val/test subsets and upload them in \"Dataset\" folder. You need to unzip it and put it in the same directory when you run this code.\n",
    "\n",
    "In this code we comment out the codes for execution in Google Colab environment and assume desktop environment. Some modules work when you want to start the training from previous epochs, so you need to pay attention to which modules you should use for your own task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dependencies\n",
    "-------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time, math, glob\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "import argparse, os\n",
    "import torch\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.utils.data as data\n",
    "import h5py\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL.ImageOps import colorize\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Colab Drive mount\n",
    "--------\n",
    "\n",
    "The codes below for Colab drive use are commented. Use them when you want to run this in Colab Environment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ev6yQVXEQ0nQ",
    "outputId": "1711af7e-b42c-4d39-813a-212779a8fc92"
   },
   "source": [
    "# from google.colab import drive\n",
    "import sys\n",
    "# drive.mount('/content/drive')\n",
    "# sys.path.append('/content/drive/MyDrive/modules')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Data\n",
    "--------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCo6f2MdQ3CZ",
    "outputId": "e3008ff1-5192-4da2-c2b5-b78cb577003b"
   },
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(),\n",
    "        # transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "working_dir = os.getcwd()\n",
    "data_dir = os.path.join(working_dir, 'Cifar10_split')\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
    "                                             shuffle=True, num_workers=8, drop_last=True)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainloaders = dataloaders['train']\n",
    "valloaders = dataloaders['val']\n",
    "testloaders = dataloaders['test']"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1rEdqZatQ7bU"
   },
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "model_name = '0820_pytorch_bicubic_lighter_network_epoch=50_lr=adjusted'\n",
    "writer = SummaryWriter(os.path.join(working_dir, 'tensorboard', model_name))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Network Structure\n",
    "---------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NHZhTJZXQ9mu"
   },
   "source": [
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 50\n",
    "N_CLASSES = 25\n",
    "\n",
    "def conv_layer(chann_in, chann_out, k_size, p_size):\n",
    "    layer = nn.Sequential(\n",
    "        nn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),\n",
    "        nn.BatchNorm2d(chann_out),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "\n",
    "def vgg_conv_block(in_list, out_list, k_list, p_list, pooling_k, pooling_s):\n",
    "\n",
    "    layers = [conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) for i in range(len(in_list))]\n",
    "    layers += [nn.MaxPool2d(kernel_size=pooling_k, stride=pooling_s)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def vgg_fc_layer(size_in, size_out):\n",
    "    layer = nn.Sequential(\n",
    "        nn.Linear(size_in, size_out),\n",
    "        nn.BatchNorm1d(size_out),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "\n",
    "class SDCNN(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(SDCNN, self).__init__()\n",
    "\n",
    "        # Conv blocks (BatchNorm + ReLU activation added in each block)\n",
    "        self.layer1 = vgg_conv_block([3,32], [32,32], [3,3], [1,1], 2, 2)\n",
    "        self.layer2 = vgg_conv_block([32,64], [64,64], [3,3], [1,1], 2, 2)\n",
    "\n",
    "        # FC layers\n",
    "        self.layer3 = vgg_fc_layer(16*16*64, 4096)  # 4096->smaller\n",
    "\n",
    "        # Final layer\n",
    "        self.layer4 = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Start from here\n",
    "        out = self.layer1(x)\n",
    "        features = self.layer2(out)\n",
    "        out = features.view(out.size(0), -1)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return features, out"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uywJoG2VQ_zB"
   },
   "source": [
    "CN = SDCNN().to(device)\n",
    "cost = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(CN.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Previous Training Stage (if interrupted)\n",
    "---------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGUsg_ucFr4M",
    "outputId": "c1f83899-acfb-449d-8074-23856fcdf446"
   },
   "source": [
    "CN.load_state_dict(torch.load(os.path.join(working_dir, 'trained_models', model_name , 'model_weights_epoch=46.pth')))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYLK1TToRAcB",
    "outputId": "b1be9b9d-2c16-44f8-f631-b517dae91ec0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train the model\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the epoch will be declined to 1/10 every 10 epoch\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    lr = 0.01 * (0.1 ** (epoch // 10))\n",
    "    return lr\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ', epoch)\n",
    "    CN.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    avg_loss = 0\n",
    "    for images, labels in trainloaders:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, outputs = CN(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = cost(outputs, labels)\n",
    "        avg_loss += float(loss.data)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step(avg_loss)\n",
    "    epoch_loss = running_loss / dataset_sizes['train']\n",
    "    epoch_acc = running_corrects.double() / dataset_sizes['train']\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                'train', epoch_loss, epoch_acc))\n",
    "    writer.add_scalar('training loss',epoch_loss,epoch)\n",
    "    writer.add_scalar('training accuracy',epoch_acc,epoch)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(CN.state_dict(), os.path.join(working_dir, 'trained_models', model_name , 'model_weights_epoch=' + str(epoch) + '.pth'))\n",
    "    \n",
    "    CN.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    with torch.no_grad():\n",
    "      for images, labels in valloaders:        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, outputs = CN(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = cost(outputs, labels)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        \n",
    "      # scheduler.step()\n",
    "      epoch_loss = running_loss / dataset_sizes['val']\n",
    "      epoch_acc = running_corrects.double() / dataset_sizes['val']\n",
    "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                'val', epoch_loss, epoch_acc))\n",
    "      writer.add_scalar('validation loss',epoch_loss,epoch)\n",
    "      writer.add_scalar('validation accuracy',epoch_acc,epoch)\n",
    "    \n",
    "    \n",
    "      lr = adjust_learning_rate(optimizer, epoch-1)\n",
    "\n",
    "      for param_group in optimizer.param_groups:\n",
    "          param_group[\"lr\"] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f56kuU1RfDn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Test the model\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CN.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in testloaders:\n",
    "    _, outputs = CN(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "    print(predicted, labels, correct, total)\n",
    "    print(\"avg acc: %f\" % (100* correct/total))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}